<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaan Akan</title>
  
  <meta name="author" content="Adil Kaan Akan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Adil Kaan Akan</name>
              </p>
              <p>I am a PhD candidate at <a target="_blank" rel="noopener noreferrer" href="https://cs.ku.edu.tr">Koc University</a>, where I work on computer vision and machine learning.
              </p>
              <p>
                At <a href="https://ai.ku.edu.tr">KUIS AI Lab</a>, I am working on weakly-supervised temporal action detection and video understanding, under the supervision of <a target="_blank" rel="noopener noreferrer" href="http://home.ku.edu.tr/~yyemez/">Prof. Yücel Yemez</a>.
              </p>
              <p>
                Previously, I recevied my Master's degree (<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2209.10693">Thesis</a>) from <a href="https://ai.ku.edu.tr">KUIS AI Lab</a>, Koc University, my Bachelor's from <a href="http://ceng.metu.edu.tr">Department of Computer Engineering</a>, Middle East Technical University. I have received "Academic Excellence Award" at Koc University for my contributions to the research and academic standing.
              </p>
              <p style="text-align:center">
                <a href="mailto:kaanakan97.ka@gmail.com">Email</a> &nbsp/&nbsp
                <a target="_blank" href="data/kaanakan-CV.pdf" rel="noopener noreferrer">CV</a> &nbsp/&nbsp
                <a target="_blank" rel="noopener noreferrer" href="data/kaanakan-bio.txt">Bio</a> &nbsp/&nbsp
                <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com.tr/citations?user=0nONNn0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/akaan_akan">Twitter</a> &nbsp/&nbsp
                <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaanakan/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/kaanakan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/kaanakan.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, with a special interest in temporal problems and generative models such as temporal action detection, future prediction, video understanding and generative models.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/slotadapt.jpeg" alt="ADAPT" height="170"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2501.15878" id="MCG_journal">
                <papertitle>Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://mysite.ku.edu.tr/yyemez/">Yücel Yemez</a>
              <br>
              <em>ICLR</em>, 2025
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2501.15878">Preprint</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://kaanakan.github.io/SlotAdapt/">Project Website</a> /
              <!-- <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaanakan/stretchbev">Code</a> / -->
              <a href="data/slotadapt.bib">bibtex</a>
              <p></p>
              <p>SlotAdapt combines slot attention with pretrained diffusion models through adapters for slot-based conditioning. By adding a guidance loss to align cross-attention with slot attention, our model better identifies objects without external supervision. Experiments show superior performance in object discovery and image generation, particularly on complex real-world images.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/adapt.gif" alt="ADAPT" height="120"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2307.14187" id="MCG_journal">
                <papertitle>ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation</papertitle>
              </a>
              <br>
              <a target="_blank" rel="noopener noreferrer" href="">Gorkay Aydemir</a> <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://mysite.ku.edu.tr/fguney/">Fatma Guney</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2307.14187">Preprint</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://kuis-ai.github.io/adapt/">Project Website</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://github.com/gorkaydemir/ADAPT">Code</a> /
              <a href="data/adapt.bib">bibtex</a>
              <p></p>
              <p>We propose a novel method for trajectory prediction that can adapt itself into every agent in the shared scene. We exploit dynamic weight learning to adapt each agent's state separately to predict their future trajectories simultaneously without rotating and normalizing the scene frame. Our results achieve state-of-the-art performance on Argoverse and INTERACTION datasets with impressive runtime performance.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stretch.gif" alt="StretchBEV" height="120"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.13641" id="MCG_journal">
                <papertitle>StretchBEV: Stretching Future Instance Prediction Spatially and Temporally</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://mysite.ku.edu.tr/fguney/">Fatma Guney</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.13641">Preprint</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://kuis-ai.github.io/stretchbev/">Project Website</a> /
              <!-- <a target="_blank" rel="noopener noreferrer" href="https://kuis-ai.github.io/slamp/">Project Website</a> / -->
              <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaanakan/stretchbev">Code</a> /
              <a href="data/stretch.bib">bibtex</a>
              <p></p>
              <p>We propose a novel method for future instance segmentation in Bird's-eye view space. We exploit state-space models for the future state prediction for encoding 3D scene structure and decoding future instance segmentations. Our results achieve state-of-the-art performance on NuScnes dataset with a great margin.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ftgn.gif" alt="FTGN" height="120"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.00255" id="MCG_journal">
                <papertitle>Trajectory Forecasting on Temporal Graphs</papertitle>
              </a>
              <br>
              <a target="_blank" rel="noopener noreferrer" href="">Gorkay Aydemir</a> <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://mysite.ku.edu.tr/fguney/">Fatma Guney</a>
              <br>
              <em>Arxiv preprint</em>, 2022
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.00255">Preprint</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://kuis-ai.github.io/ftgn/">Project Website</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://github.com/gorkaydemir/ftgn">Code</a> /
              <a href="data/ftgn_arxiv.bib">bibtex</a>
              <p></p>
              <p>We propose a novel method for trajectory prediction. We propose to use Temporal Graph Networks for learning dynamically evolving agent features. Our results reaches the state-of-the-art performance on Argoverse Forecasting dataset.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/slamp3d.gif" alt="SLAMP3D" height="120"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.10528" id="MCG_journal">
                <papertitle>Stochastic Video Prediction with Structure and Motion</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://tr.linkedin.com/in/sadra-safadoust">Sadra Safadoust</a>, <a target="_blank" rel="noopener noreferrer" href="https://mysite.ku.edu.tr/fguney/">Fatma Guney</a>
              <br>
              <em>Arxiv preprint</em>, 2022
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.10528">Preprint</a> /
              <!-- <a target="_blank" rel="noopener noreferrer" href="https://kuis-ai.github.io/slamp/">Project Website</a> / -->
              <!-- <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaanakan/slamp">Code</a> / -->
              <a href="data/slamp3d_arxiv.bib">bibtex</a>
              <p></p>
              <p>We decompose the scene into static and dynamic parts by encoding it into ego-motion and optical flow. We first factorize scene structure, the ego-motion, then conditioned on this, we predict the residual flow in the scene specifically for independently moving objects.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/slamp.gif" alt="SLAMP" height="120"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/ICCV2021/html/Akan_SLAMP_Stochastic_Latent_Appearance_and_Motion_Prediction_ICCV_2021_paper.html" id="MCG_journal">
                <papertitle>SLAMP: Stochastic Latent Appearance and Motion Prediction</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://web.cs.hacettepe.edu.tr/~erkut/">Erkut Erdem</a>, <a target="_blank" rel="noopener noreferrer" href="https://aykuterdem.github.io/">Aykut Erdem</a>, <a target="_blank" rel="noopener noreferrer" href="https://mysite.ku.edu.tr/fguney/">Fatma Guney</a>
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.02760">Preprint</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://kuis-ai.github.io/slamp/">Project Website</a> /
              <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaanakan/slamp">Code</a> /
              <a href="data/slamp.bib">bibtex</a>
              <p></p>
              <p>We propose a novel way for stochastic video prediction by decomposing static and dynamic parts of the scene. We reason about appearance and motion in the video stochastically by predicting the future based on the motion history.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/extended.png" alt="JNDJournal" height="120"  width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://link.springer.com/article/10.1007/s11760-021-02114-x" id="MCG_journal">
                <papertitle>Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="https://user.ceng.metu.edu.tr/~emre/">Emre Akbas</a>, <a target="_blank" rel="noopener noreferrer" href="https://vural.ceng.metu.edu.tr/">Fatos T. Yarman-Vural</a>
              <br>
              <em>Signal, Image and Video Processing</em>, 2021
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.08079">Preprint</a> /
              <a href="data/extended.bib">bibtex</a>
              <p></p>
              <p>We propose theoretical understanding of JND concept for machine perception and conduct further analyses and comparisons with other state-of-the-art methods.</p>
              <p>This paper extends our ICIP 2020 paper.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jnd.png" alt="JND" height="120" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/9191090" id="MCG_journal">
                <papertitle>Just Noticeable Difference for Machines to Generate Adversarial Images</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, <a target="_blank" rel="noopener noreferrer" href="http://mehmetaligenc.com/">Mehmet Ali Genc</a>, <a target="_blank" rel="noopener noreferrer" href="https://vural.ceng.metu.edu.tr/">Fatos T. Yarman-Vural</a>
              <br>
              <em>IEEE International Conference on Image Processing</em>, (ICIP) 2020
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.11064">Preprint</a> /
              <a href="data/jnd.bib">bibtex</a>
              <p></p>
              <p>We propose a new concept for adversarial example generation. Inspired by the experimental psychology, we use the concept of Just Noticeable Difference to generate natural looking adverarial images.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tol.png" alt="b-ann" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/8806456" id="MCG_journal">
                <papertitle>Modeling and Decoding Complex Problem Solving Process by Artificial Neural Networks</papertitle>
              </a>
              <br>
              <strong>Adil Kaan Akan</strong>, Baran Baris Kivilcim, <a href="https://user.ceng.metu.edu.tr/~emre/">Emre Akbas</a>, Sharlene D. Newman, <a href="https://vural.ceng.metu.edu.tr/">Fatos T. Yarman-Vural</a>
              <br>
              <em>Signal Processing and Communications Applications Conference</em>, (SIU) 2019
              <br>
              <a href="data/fmri.bib">bibtex</a>
              <p></p>
              <p>We proposed a new method for classifying cognitive states using Artificial Brain Networks. The proposed model generates state-of-the-art results for Tower of London, complex problem solving dataset.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
        </p><ul>
          <li>COMP302: Software Engineering, Koc University</li>
          <li>COMP100: Introduction to Computer Science and Programming, Koc University</li>
          <li>CENG223: Discrete Computational Structures, Middle East Technical University</li>
          <li>CENG230: C Programming, Middle East Technical University</li>
        </ul>
                  <p></p>
                </td>
              </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website format from <a href="https://jonbarron.info/">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
